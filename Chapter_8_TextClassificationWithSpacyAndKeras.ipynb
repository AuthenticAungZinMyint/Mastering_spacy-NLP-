{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49c134d",
   "metadata": {},
   "source": [
    " Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b0d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"Tomorrow I will visit the hospital.\",\n",
    "    \"Yesterday I took a flight to Athens.\",\n",
    "    \"Sally visited Harry and his dog.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc48846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tomorrow', 'I', 'will', 'visit', 'the', 'hospital', '.']\n",
      "['Yesterday', 'I', 'took', 'a', 'flight', 'to', 'Athens', '.']\n",
      "['Sally', 'visited', 'Harry', 'and', 'his', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenize the word into sentences\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "sentences = [[token.text for token in nlp(sentence)] for sentence in data]\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b271e113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x250a8b7f4f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras txt-preprocess turn word sequence into word id seq with tokenizer class\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7b6fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'tomorrow': 2,\n",
       " 'will': 3,\n",
       " 'visit': 4,\n",
       " 'the': 5,\n",
       " 'hospital': 6,\n",
       " 'yesterday': 7,\n",
       " 'took': 8,\n",
       " 'a': 9,\n",
       " 'flight': 10,\n",
       " 'to': 11,\n",
       " 'athens': 12,\n",
       " 'sally': 13,\n",
       " 'visited': 14,\n",
       " 'harry': 15,\n",
       " 'and': 16,\n",
       " 'his': 17,\n",
       " 'dog': 18}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "288e2420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"hospital\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ed801f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6], [8]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"hospital\", \"took\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcf30925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will tomorrow i']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[3,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3077d479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will tomorrow i', 'the hospital flight']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[3,2,1], [5,6,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c91b4da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  0,  0,  0],\n",
       "       [ 8,  1,  0,  0],\n",
       "       [ 9, 11, 12, 14]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 keras padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = [[7], [8,1], [9,11,12,14]]\n",
    "MAX_LEN=4\n",
    "pad_sequences(sequences, MAX_LEN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e4a335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  7],\n",
       "       [ 0,  0,  8,  1],\n",
       "       [ 9, 11, 12, 14]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(sequences, MAX_LEN, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "997c9ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1,  3,  4,  5,  6,  0],\n",
       "       [ 7,  1,  8,  9, 10, 11, 12],\n",
       "       [13, 14, 15, 16, 17, 18,  0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "seqs = tokenizer.texts_to_sequences(data)\n",
    "MAX_LEN = 7\n",
    "\n",
    "padded_seqs = pad_sequences(seqs, MAX_LEN, padding=\"post\")\n",
    "padded_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e64bcc",
   "metadata": {},
   "source": [
    "    Embedding words\n",
    "    1.We broke each sentence into words and built a vocabulary with Keras' Tokenizer. \n",
    "    2.The Tokenizer object held a word index, which was a word->word-ID mapping. \n",
    "    3.After obtaining the word-ID, we could do a lookup to the embedding table rows with this word-ID and got a word vector. \n",
    "    4.Finally, we fed this word vector to the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751ebd3",
   "metadata": {},
   "source": [
    "    Neural Network architecture for text classification\n",
    "  1. we'll first preprocess, tokenize, pad the review sentences and after this, we will obtain a list of sequence\n",
    "  2. we'll feed this list to the neural network through input layer.\n",
    "  3. we'll vectorize each word by looking its word-id in the embadding layer. at this point, a sentence is now a sequence of word vectors, each vectors correspond to a word\n",
    "  4. next, we will feed the seq of word vectors to LSTM.\n",
    "  5. finally, we'll squash the LSTM output with a sigmoid layer to obtain class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d74f3400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50057</td>\n",
       "      <td>B000ER5DFQ</td>\n",
       "      <td>A1ESDLEDR9Y0JX</td>\n",
       "      <td>A. Spencer</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1310256000</td>\n",
       "      <td>the garbanzo beans in it give horrible gas</td>\n",
       "      <td>To be fair only one of my twins got gas from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366917</td>\n",
       "      <td>B001AIQP8M</td>\n",
       "      <td>A324KM3YY1DWQG</td>\n",
       "      <td>danitrice</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1251072000</td>\n",
       "      <td>Yummy Lil' Treasures!!</td>\n",
       "      <td>Just recieved our first order of these (they d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214380</td>\n",
       "      <td>B001E5E1XW</td>\n",
       "      <td>A3QCWO53N69HW3</td>\n",
       "      <td>M. A. Vaughan \"-_-GOBNOGO-_-\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1276473600</td>\n",
       "      <td>Great Chai</td>\n",
       "      <td>This is a fantastic Chai Masala. I am very pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178476</td>\n",
       "      <td>B000TIZP5I</td>\n",
       "      <td>AYZ5NG9705AG1</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1341360000</td>\n",
       "      <td>Celtic Salt worth extra price</td>\n",
       "      <td>Flavorful and has added nutrition!  You use le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>542504</td>\n",
       "      <td>B000E18CVE</td>\n",
       "      <td>A2LMWCJUF5HZ4Z</td>\n",
       "      <td>Miki Lam \"mikilam\"</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1222732800</td>\n",
       "      <td>mixed feelings</td>\n",
       "      <td>I thought this soup tasted good. I liked the t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId                    ProfileName  \\\n",
       "0   50057  B000ER5DFQ  A1ESDLEDR9Y0JX                     A. Spencer   \n",
       "1  366917  B001AIQP8M  A324KM3YY1DWQG                      danitrice   \n",
       "2  214380  B001E5E1XW  A3QCWO53N69HW3  M. A. Vaughan \"-_-GOBNOGO-_-\"   \n",
       "3  178476  B000TIZP5I   AYZ5NG9705AG1                       Consumer   \n",
       "4  542504  B000E18CVE  A2LMWCJUF5HZ4Z             Miki Lam \"mikilam\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       2      1  1310256000   \n",
       "1                     0                       0      5  1251072000   \n",
       "2                     2                       2      5  1276473600   \n",
       "3                     0                       0      5  1341360000   \n",
       "4                     8                      11      3  1222732800   \n",
       "\n",
       "                                      Summary  \\\n",
       "0  the garbanzo beans in it give horrible gas   \n",
       "1                      Yummy Lil' Treasures!!   \n",
       "2                                  Great Chai   \n",
       "3               Celtic Salt worth extra price   \n",
       "4                              mixed feelings   \n",
       "\n",
       "                                                Text  \n",
       "0  To be fair only one of my twins got gas from t...  \n",
       "1  Just recieved our first order of these (they d...  \n",
       "2  This is a fantastic Chai Masala. I am very pic...  \n",
       "3  Flavorful and has added nutrition!  You use le...  \n",
       "4  I thought this soup tasted good. I liked the t...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import realword data (Amazon customers' food reviews)\n",
    "import pandas as pd\n",
    "\n",
    "reviews_df = pd.read_csv(\"data/reviews.csv\")\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa5b8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df[['Text', 'Score']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8692bf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To be fair only one of my twins got gas from t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just recieved our first order of these (they d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a fantastic Chai Masala. I am very pic...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flavorful and has added nutrition!  You use le...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I thought this soup tasted good. I liked the t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score\n",
       "0  To be fair only one of my twins got gas from t...      1\n",
       "1  Just recieved our first order of these (they d...      5\n",
       "2  This is a fantastic Chai Masala. I am very pic...      5\n",
       "3  Flavorful and has added nutrition!  You use le...      5\n",
       "4  I thought this soup tasted good. I liked the t...      3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49b5c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp/ipykernel_9888/1369280456.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews_df.Score[reviews_df.Score <=3 ] = 0\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp/ipykernel_9888/1369280456.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews_df.Score[reviews_df.Score >=4 ] = 1\n"
     ]
    }
   ],
   "source": [
    "reviews_df.Score[reviews_df.Score <=3 ] = 0\n",
    "reviews_df.Score[reviews_df.Score >=4 ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d12eb858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To be fair only one of my twins got gas from t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just recieved our first order of these (they d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a fantastic Chai Masala. I am very pic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flavorful and has added nutrition!  You use le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I thought this soup tasted good. I liked the t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score\n",
       "0  To be fair only one of my twins got gas from t...      0\n",
       "1  Just recieved our first order of these (they d...      1\n",
       "2  This is a fantastic Chai Masala. I am very pic...      1\n",
       "3  Flavorful and has added nutrition!  You use le...      1\n",
       "4  I thought this soup tasted good. I liked the t...      0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "629fc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "labels = []\n",
    "\n",
    "for index, row in reviews_df.iterrows():\n",
    "    text = row['Text']\n",
    "    rating = row['Score']\n",
    "    labels.append(rating)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    train_examples.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88dab4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and vocabulary preparation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(train_examples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_examples)\n",
    "\n",
    "MAX_LEN = 50\n",
    "X = pad_sequences(sequences, MAX_LEN, padding=\"post\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25c0d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ready to feed our data to neural network\n",
    "#feed data to the input layers\n",
    "#import necessaries\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# The input layer\n",
    "sentence_input = Input(shape=(None,))\n",
    "\n",
    "# The embedding layer\n",
    "#input_dim = number of words in vocab, +1 = indices start from 1 not 0\n",
    "#output shape 100 dims, popular no for word vector dim are 50, 200, 100\n",
    "embedding = Embedding(input_dim = len(tokenizer.word_index)+1,\n",
    "                     output_dim = 100)(sentence_input)\n",
    "\n",
    "#The LSTM layer\n",
    "#unit params means the dim of hidden state(lstm hidden and output shape are same)\n",
    "LSTM_layer = LSTM(units=256)(embedding)\n",
    "\n",
    "#The output layer\n",
    "#squash 256-dim vector from lstm to 1-dim\n",
    "#sigmoid function is S shape function and map its input to [0-1] range\n",
    "output_dense = Dense(1, activation='sigmoid')(LSTM_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "caebfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "model = Model(inputs=[sentence_input],outputs=[output_dense])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ec00adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam(adaptive moment estimation), a popular optimizer in deep-learning\n",
    "#binary cross-entropy, a loos that is used in binaru classification\n",
    "#metrics use to evalute the performance of model\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16a11cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 23s 342ms/step - loss: 0.5430 - accuracy: 0.7727 - val_loss: 0.5284 - val_accuracy: 0.7563\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 16s 310ms/step - loss: 0.3939 - accuracy: 0.8249 - val_loss: 0.4975 - val_accuracy: 0.7950\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 15s 309ms/step - loss: 0.2088 - accuracy: 0.9259 - val_loss: 0.5829 - val_accuracy: 0.7987\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 0.1089 - accuracy: 0.9684 - val_loss: 0.6502 - val_accuracy: 0.7875\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 15s 309ms/step - loss: 0.0497 - accuracy: 0.9859 - val_loss: 0.8621 - val_accuracy: 0.7925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x250b7ad65b0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model and experiment evaluation\n",
    "model.fit(x=X, \n",
    "          y=y, \n",
    "          batch_size=64, \n",
    "          epochs=5, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf696a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
