{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3c907a",
   "metadata": {},
   "source": [
    "    Token Based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5070785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4843b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Good morning,\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Good morning, I want to reserve a ticket.')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"good\"}, {\"LOWER\": \"morning\"}, {\"IS_PUNCT\": True}]\n",
    "matcher.add(\"morningGreeting\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    m_span = doc[start:end]\n",
    "    print(start, end, m_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ec35232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lexeme.Lexeme object at 0x000001BB2A4EF3C0> 0 3 Good morning,\n",
      "<spacy.lexeme.Lexeme object at 0x000001BBA94F88C0> 14 17 good evening!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"Good morning, I want to resverse a ticket. I will then say good evening!\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"good\"}, {\"LOWER\": \"morning\"}, {\"IS_PUNCT\": True}]\n",
    "matcher.add(\"morningGreeting\", [pattern1])\n",
    "pattern2 = [{\"LOWER\": \"good\"}, {\"LOWER\": \"evening\"}, {\"IS_PUNCT\": True}]\n",
    "matcher.add(\"eveningGreeting\", [pattern2])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    pattern_name = nlp.vocab[match_id]\n",
    "    m_span = doc[start:end]\n",
    "    print(pattern_name, start, end, m_span.text)\n",
    "             \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7dc9e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 I\n",
      "2 3 a\n",
      "4 5 .\n"
     ]
    }
   ],
   "source": [
    "#length\n",
    "doc = nlp(\"I bought a pineapple.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LENGTH\": 1}]\n",
    "matcher.add(\"onlyShort\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe4a440b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I met him at 2 o'clock."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IS_ALPHA, IS_ASCII, IS_DIGIT\n",
    "doc1 = nlp(\"I met him at 2 o'clock.\")\n",
    "doc2 = nlp(\"He brought me 2 apples.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"IS_DIGIT\": True},{\"IS_ALPHA\": True}]\n",
    "matcher.add(\"numberAndPlainWord\", [pattern])\n",
    "\n",
    "matches = matcher(doc1)\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39aebc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc2)\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5d6d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5 2 apples\n"
     ]
    }
   ],
   "source": [
    "mid, start, end = matches[0]\n",
    "print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c8a6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 SPAM\n",
      "22 23 SUE\n"
     ]
    }
   ],
   "source": [
    "#IS_UPPER, IS_LOWER, IS_TITLE\n",
    "\n",
    "doc = nlp(\"Take me out of your SPAM list. We never asked you to conatact me. If you write again we'll SUE!!!\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"IS_UPPER\": True}]\n",
    "matcher.add(\"capitals\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "755f774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IS_PUNCT, IS_SPACE, IS_STOP\n",
    "doc1 = nlp(\"Can you swim?\")\n",
    "doc2 = nlp(\"Can Sally swim?\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"IS_SENT_START\": True, \"LOWER\": \"can\"}, {\"IS_TITLE\": True}]\n",
    "\n",
    "matcher.add(\"canThenCapitalized\", [pattern])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bedddbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc2)\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58052e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 Can Sally\n"
     ]
    }
   ],
   "source": [
    "mid, start, end = matches[0]\n",
    "print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e8bc899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Will you go there?\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"IS_SENT_START\": True, \"TAG\": \"MD\"}]\n",
    "matcher.add(\"tagM\", [pattern])\n",
    "matches = matcher(doc)\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f974879a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 Will\n"
     ]
    }
   ],
   "source": [
    "mid, start, end = matches[0]\n",
    "print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51c3f223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(\"I might go there.\")\n",
    "matcher(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdcd119",
   "metadata": {},
   "source": [
    "    Extended Syntax Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa557d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Good morning,\n",
      "10 13 good evening!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Good morning, I'm here. I'll say good evening!!\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"good\"}, {\"LOWER\": {\"IN\": [\"morning\", \"evening\"]}},\n",
    "          {\"IS_PUNCT\": True}]\n",
    "matcher.add(\"greeting\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0d63594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 Trichotillomania\n",
      "14 15 Psychosomatic\n"
     ]
    }
   ],
   "source": [
    "#comparison operator with length\n",
    "doc = nlp(\"I suffered from Trichotillomania when I was in college. The doctor precribed me Psychosomatic medicine.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LENGTH\": {\">=\" : 10}}]\n",
    "matcher.add(\"longWords\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04dda1",
   "metadata": {},
   "source": [
    "    Regex-like operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4e6659c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9957319642918298529, 0, 2)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(\"Barack Obama visited France.\")\n",
    "doc2 = nlp(\"Barack Hussein Obama visited France.\")\n",
    "pattern = [{\"LOWER\": \"barack\"},\n",
    "          {\"LOWER\": \"hussein\", \"OP\": \"?\"},\n",
    "          {\"LOWER\": \"obama\"}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"obamaNames\", [pattern])\n",
    "matcher(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ec591cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9957319642918298529, 0, 3)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "72123cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 Hello hello hello,\n",
      "1 4 hello hello,\n",
      "2 4 hello,\n",
      "3 4 ,\n",
      "7 8 ?\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Hello hello hello, how are you?\")\n",
    "doc2 = nlp(\"Hello, how are you?\")\n",
    "doc3 = nlp(\"How are you?\")\n",
    "pattern = [{\"LOWER\": {\"IN\": [\"hello\", \"hi\", \"hallo\"]},\"OP\" : \"*\"}, {\"IS_PUNCT\": True}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"greetings\", [pattern])\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f6f7026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 Hello,\n",
      "1 2 ,\n",
      "5 6 ?\n"
     ]
    }
   ],
   "source": [
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "655c7004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 ?\n"
     ]
    }
   ],
   "source": [
    "for mid, start, end in matcher(doc3):\n",
    "    print(start, end, doc3[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2f27cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 hello,\n",
      "1 4 hello hello,\n",
      "0 4 Hello hello hello,\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Hello hello hello, how are you?\")\n",
    "doc2 = nlp(\"Hello, how are you?\")\n",
    "doc3 = nlp(\"How are you?\")\n",
    "pattern = [{\"LOWER\": {\"IN\": [\"hello\", \"hi\", \"hallo\"]},\"OP\" : \"+\"}, {\"IS_PUNCT\": True}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"greetings\", [pattern])\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ed4abe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 Hello,\n"
     ]
    }
   ],
   "source": [
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4b348e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mid, start, end in matcher(doc3):\n",
    "    print(start, end, doc3[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f498697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 name is Alice\n",
      "6 9 name was Elliot\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"My name is Alice and his name was Elliot.\")\n",
    "pattern = [{\"LOWER\": \"name\"},{\"LEMMA\": \"be\"}, {}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"pickName\", [pattern])\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "36558bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 forwarded his email\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I forwarded his email to you.\")\n",
    "doc2 = nlp(\"I forwarded an email to you.\")\n",
    "doc3 = nlp(\"I forwarded the email to you.\")\n",
    "\n",
    "pattern = [{\"LEMMA\": \"forward\"}, {}, {\"LOWER\": \"email\"}]\n",
    "matcher.add(\"forwardMail\", [pattern])\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb374eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 forwarded an email\n"
     ]
    }
   ],
   "source": [
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cab906db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 forwarded the email\n"
     ]
    }
   ],
   "source": [
    "for mid, start, end in matcher(doc3):\n",
    "    print(start, end, doc3[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a0423",
   "metadata": {},
   "source": [
    "    Regex Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab21a535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 I travelled\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I travelled by bus.\")\n",
    "doc2 = nlp(\"She travelled by bike.\")\n",
    "pattern = [{\"POS\": \"PRON\"}, \n",
    "          {\"TEXT\": {\"REGEX\": \"[Tt]ravell?ed\"}}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"regexSupport\", [pattern])\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d2b8aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 She travelled\n"
     ]
    }
   ],
   "source": [
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b78fb0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 went\n",
      "6 7 has\n",
      "7 8 been\n",
      "14 15 has\n",
      "15 16 told\n",
      "18 19 wants\n",
      "20 21 visit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I went to Italy; he has been there too. His mother also has told me she wants to visit Rome.\")\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
    "matcher.add(\"verbs\", [pattern])\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f322162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/sherlock_holmes_1.txt\"\n",
    "file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7f8ea466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5 is\n",
      "12 13 have\n",
      "14 15 heard\n",
      "17 18 mention\n",
      "28 29 eclipses\n",
      "31 32 predominates\n",
      "39 40 was\n",
      "43 44 felt\n",
      "63 64 were\n",
      "77 78 was\n",
      "80 81 take\n",
      "88 89 observing\n",
      "94 95 has\n",
      "95 96 seen\n",
      "103 104 have\n",
      "104 105 placed\n",
      "114 115 spoke\n",
      "120 121 save\n",
      "123 124 gibe\n",
      "130 131 were\n",
      "140 141 drawing\n",
      "153 154 trained\n",
      "157 158 admit\n",
      "167 168 adjusted\n",
      "169 170 was\n",
      "171 172 introduce\n",
      "173 174 distracting\n",
      "178 179 throw\n",
      "210 211 be\n",
      "228 229 was\n",
      "238 239 was\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
    "matcher.add(\"verbs\", [pattern])\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "    PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3babc93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 11 Angela Merkel\n",
      "16 18 Donald Trump\n",
      "22 24 Alexis Tsipras\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "terms = ['Angela Merkel', 'Donald Trump', 'Alexis Tsipras']\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"politicianList\", None, *patterns)\n",
    "\n",
    "doc = nlp(\"3 EU leaders met in Berlin. German chancellor Angela Merkel first welcomed the US president Donald Trump. The following day Alexis Tsipras joined them in Brandenburg.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f7b5ff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 derivatives\n",
      "6 7 market\n",
      "9 10 asset\n"
     ]
    }
   ],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "terms = [\"Asset\", \"Investment\", \"Derivatives\", \"Demand\", \"Market\"]\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"financeTerms\", None, *patterns)\n",
    "doc = nlp(\"During the last decade, derivatives market became an asset class of their own and influenced the financial landscape strongly.\")\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b4522134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 9 192.1.1.1\n",
      "12 13 192.160.1.1\n"
     ]
    }
   ],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "ip_nums = [\"127.0.0.0\", \"127.256.0.0\"]\n",
    "patterns = [nlp.make_doc(ip) for ip in ip_nums]\n",
    "matcher.add(\"IPNums\", None, *patterns)\n",
    "\n",
    "doc = nlp(\"This log contains the following IP addresses: 192.1.1.1 and 192.12.1.1 and 192.160.1.1 .\")\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c51be",
   "metadata": {},
   "source": [
    "    Entity Ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a648dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 Bill\n",
      "1 2 Gates\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"ENT_TYPE\": \"PERSON\"}]\n",
    "matcher.add('personEnt', [pattern])\n",
    "doc = nlp(\"Bill Gates visited Berlin.\")\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f63a8932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 Bill\n",
      "1 2 Gates\n",
      "0 2 Bill Gates\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"ENT_TYPE\": \"PERSON\", \"OP\": \"+\"}]\n",
    "matcher.add('personEnt', [pattern])\n",
    "doc = nlp(\"Bill Gates visited Berlin.\")\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35a5f09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 Merkel met\n",
      "3 6 Angela Merkel met\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"ENT_TYPE\": \"PERSON\", \"OP\": \"+\"},\n",
    "          {\"POS\": \"VERB\"}]\n",
    "matcher.add(\"personEntAction\", [pattern])\n",
    "doc = nlp(\"Today German chancellor Angela Merkel met with the US president.\")\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6071ddd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2017,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I have an account with chime since 2017.\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71c0ce27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chime, 2017)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import pipeline\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "patterns = [{\"label\": \"ORG\",\n",
    "            \"pattern\": [{\"LOWER\": \"chime\"}]}]\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(\"I have an account with chime since 2017.\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d2311",
   "metadata": {},
   "source": [
    "    Combining spacy models and matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf632d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 BE71 0961\n",
      "4 7 BE71 0961 2345\n",
      "4 8 BE71 0961 2345 6769\n"
     ]
    }
   ],
   "source": [
    "#Extracting IBAN and account numbers\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"My IBAN number is BE71 0961 2345 6769, please send the money there.\")\n",
    "doc1 = nlp(\"My IBAN number is FR76 3000 6000 0112 3456 7890 189, please send the money there.\")\n",
    "\n",
    "pattern = [{\"SHAPE\": \"XXdd\"},\n",
    "           {\"TEXT\": {\"REGEX\": \"\\d{1,4}\"},\n",
    "           \"OP\": \"+\"}]\n",
    "matcher.add(\"ibanNum\", [pattern])\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a5d643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 FR76 3000\n",
      "4 7 FR76 3000 6000\n",
      "4 8 FR76 3000 6000 0112\n",
      "4 9 FR76 3000 6000 0112 3456\n",
      "4 10 FR76 3000 6000 0112 3456 7890\n",
      "4 11 FR76 3000 6000 0112 3456 7890 189\n"
     ]
    }
   ],
   "source": [
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ceca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5 account number is 8891273\n",
      "1 5 account num is 8891273\n",
      "1 5 account no is 8891273\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"LOWER\": \"account\"}, \n",
    "           {\"LOWER\": {\"IN\": [\"num\", \"number\", \"no\"]}}, \n",
    "           {}, \n",
    "           {\"IS_DIGIT\": True}]\n",
    "doc1 = nlp(\"My account number is 8891273\")\n",
    "doc2 = nlp(\"My account num is 8891273\")\n",
    "doc3 = nlp(\"My account no is 8891273\")\n",
    "\n",
    "matcher.add(\"accountNum\", [pattern])\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])\n",
    "for mid, start, end in matcher(doc3):\n",
    "    print(start, end, doc3[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a92fdb",
   "metadata": {},
   "source": [
    "    Extracting Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1b2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "pattern = [{\"TEXT\": \"+1\", \"OP\": \"?\"},\n",
    "          {\"TEXT\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"TEXT\": \")\"},\n",
    "          {\"SHAPE\": \"ddd\"}, \n",
    "          {\"TEXT\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7eb9bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 13 +1 (221) 103-2423\n",
      "7 13 (221) 103-2423\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"You can call my office on +1 (221) 103-2423 or email me directly.\")\n",
    "doc2 = nlp(\"You can call me on (221) 102 2423 or text me.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"usPhoneNum\", [pattern])\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f62f6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10 (221) 102 2423\n"
     ]
    }
   ],
   "source": [
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b904ad",
   "metadata": {},
   "source": [
    "    Extracting Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e3aa497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "pattern = [{\"ENT_TYPE\": \"ORG\"}, {\"LEMMA\": \"be\"},\n",
    "          {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "          {\"POS\": \"ADJ\"}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"extractMentions\", [pattern])\n",
    "doc1 = nlp(\"ACME is so expensive, stay away!\")\n",
    "doc2 = nlp(\"ACME is good though price are expensive but they are worth buying.\")\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e5a52",
   "metadata": {},
   "source": [
    "    Hashtag and emoji extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e258d7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'MySpace']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"#MySpace\")\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3f90475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 #WeekendShred\n"
     ]
    }
   ],
   "source": [
    "#hashtag extract\n",
    "doc = nlp(\"Starting working out now #WeekendShred\")\n",
    "\n",
    "pattern = [{\"TEXT\": \"#\"}, {\"IS_ASCII\": True}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"hashTag\", [pattern])\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f00c0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emoji extract\n",
    "pos_emoji = [\"üòä\", \"üòÅ\", \"üòÇ\", \"üòÑ\", \"üòâ\", \"üòò\"]\n",
    "neg_emoji = [\"üòí\", \"üòè\", \"üôÉ\", \"üò§\", \"üò¨\", \"üòñ\"]\n",
    "\n",
    "pos_patterns = [{\"ORTH\": emoji} for emoji in pos_emoji]\n",
    "neg_patterns = [{\"ORTH\": emoji} for emoji in neg_emoji]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"posEmoji\", [pos_patterns])\n",
    "matcher.add(\"negEmoji\", [neg_patterns])\n",
    "\n",
    "doc = nlp(\" I love Zara üòç.\")\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372bb6a1",
   "metadata": {},
   "source": [
    " Expanding named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf0db475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Smith, 2 hours ago)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Ms.Smith left her house 2 hours ago.\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee6e808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ms.', 'TITLE'), ('Smith', 'PERSON'), ('2017', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import pipeline\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "patterns = [{\"label\": \"TITLE\", \"pattern\":\n",
    "            [{\"LOWER\": {\"IN\": [\"ms.\", \"mr.\", \"mrs.\", \"prof.\", \"dr.\"]}}]}]\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(\"Ms.Smith have an account with chime since 2017.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f824de",
   "metadata": {},
   "source": [
    " Combining linguistic features and named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bdf4644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"da3a5b2f5e8b4981886e65f3f3c7cf70-0\" class=\"displacy\" width=\"650\" height=\"212.0\" direction=\"ltr\" style=\"max-width: none; height: 212.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Einstein</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">lived</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">Zurich.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-da3a5b2f5e8b4981886e65f3f3c7cf70-0-0\" stroke-width=\"2px\" d=\"M70,77.0 C70,2.0 200.0,2.0 200.0,77.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-da3a5b2f5e8b4981886e65f3f3c7cf70-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,79.0 L62,67.0 78,67.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-da3a5b2f5e8b4981886e65f3f3c7cf70-0-1\" stroke-width=\"2px\" d=\"M220,77.0 C220,2.0 350.0,2.0 350.0,77.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-da3a5b2f5e8b4981886e65f3f3c7cf70-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350.0,79.0 L358.0,67.0 342.0,67.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-da3a5b2f5e8b4981886e65f3f3c7cf70-0-2\" stroke-width=\"2px\" d=\"M370,77.0 C370,2.0 500.0,2.0 500.0,77.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-da3a5b2f5e8b4981886e65f3f3c7cf70-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M500.0,79.0 L508.0,67.0 492.0,67.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "doc = nlp(\"Einstein lived in Zurich.\")\n",
    "[(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "displacy.render(doc, style=\"dep\", jupyter= True, options={'distance': 150}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5f31be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Einstein, 'city': [Zurich], 'past': True}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Einstein lived in Zurich.\")\n",
    "person_ents = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "for person_ent in person_ents:\n",
    "    #use head of the entity's last token\n",
    "    head = person_ent[-1].head\n",
    "    if head.lemma_ == \"live\":\n",
    "        #check if the children of live contain prepositional attachment\n",
    "        preps = [token for token in head.children\n",
    "                if token.dep_ == \"prep\" ]\n",
    "        for prep in preps:\n",
    "            places = [token for token in prep.children \n",
    "                      if token.ent_type_ == \"GPE\"]\n",
    "            #verb is in past or present tense\n",
    "            print({\"person\": person_ent, \"city\": places,\n",
    "                  \"past\": head.tag_ == \"VBD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a731de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
